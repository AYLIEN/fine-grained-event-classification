{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from pprint import pprint\n",
    "import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "\n",
    "def read_event_dataset(path):\n",
    "    with open(path) as f:\n",
    "        dataset = []\n",
    "        for line in list(f)[1:]:\n",
    "            id, text, label = line.strip().split(\"\\t\")\n",
    "            item = {\n",
    "                \"id\": id, \"text\": text, \"label\": label\n",
    "            }\n",
    "            dataset.append(item)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = Path(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_event_dataset(DIR / \"test_set_final_release_with_labels.tsv\")\n",
    "texts = [x[\"text\"] for x in dataset]\n",
    "y_true = [x[\"label\"] for x in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DIR / \"acled_label_to_name.json\") as f:\n",
    "    label_to_text = json.load(f)\n",
    "    \n",
    "label_names = sorted(label_to_text)\n",
    "label_texts = [label_to_text[l] for l in label_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZS_LABELS = [\"ORG_CRIME\", \"NATURAL_DISASTER\", \"MAN_MADE_DISASTER\", \"DIPLO\", \"ATTRIB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Simple Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_cosine(\n",
    "        model,\n",
    "        label_names,\n",
    "        label_texts=None,\n",
    "        input_texts=None,\n",
    "        input_embeddings=None,\n",
    "        label_embeddings=None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    * label_names: official names/ids of labels that are outputted as predictions\n",
    "    * label_texts: texts to build label representations from\n",
    "    \"\"\"\n",
    "\n",
    "    if label_embeddings is None:\n",
    "        label_embeddings = model.encode(label_texts)\n",
    "\n",
    "    if input_embeddings is None:\n",
    "        input_embeddings = model.encode(input_texts)\n",
    "        \n",
    "    S = util.pytorch_cos_sim(input_embeddings, label_embeddings)    \n",
    "    predicted_labels = []\n",
    "    \n",
    "    for i in range(len(input_embeddings)):\n",
    "        label_scores = S[i]                \n",
    "        scored = sorted(zip(label_names, label_scores), key=lambda x: x[1], reverse=True)\n",
    "        pred = scored[0][0]\n",
    "        predicted_labels.append(pred)        \n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "def evaluate(true_labels, pred_labels, label_set=None):\n",
    "    for avg in [\"micro\", \"macro\", \"weighted\"]:        \n",
    "        p, r, f, _ = precision_recall_fscore_support(true_labels, pred_labels, average=avg, labels=label_set)\n",
    "        gap = \" \" * (9 - len(avg))\n",
    "        print(f\"{avg}{gap}precision: {p:.3f}, recall: {r:.3f}, f-score: {f:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting and Evaluating Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 1.2.0, however, your version is 1.1.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"paraphrase-mpnet-base-v2\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = classify_with_cosine(\n",
    "    model,\n",
    "    label_names,\n",
    "    label_texts=label_texts,\n",
    "    input_texts=texts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1019"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro    precision: 0.520, recall: 0.520, f-score: 0.520\n",
      "macro    precision: 0.528, recall: 0.495, f-score: 0.461\n",
      "weighted precision: 0.569, recall: 0.520, f-score: 0.489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demian/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_true, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro    precision: 0.782, recall: 0.358, f-score: 0.491\n",
      "macro    precision: 0.871, recall: 0.383, f-score: 0.467\n",
      "weighted precision: 0.870, recall: 0.358, f-score: 0.431\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_true, predicted_labels, label_set=ZS_LABELS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
