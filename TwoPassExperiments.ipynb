{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from pprint import pprint\n",
    "import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "\n",
    "def read_event_dataset(path):\n",
    "    with open(path) as f:\n",
    "        dataset = []\n",
    "        for line in list(f)[1:]:\n",
    "            id, text, label = line.strip().split(\"\\t\")\n",
    "            item = {\n",
    "                \"id\": id, \"text\": text, \"label\": label\n",
    "            }\n",
    "            dataset.append(item)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = Path(\"data\")\n",
    "\n",
    "dataset = read_event_dataset(DIR / \"test_set_final_release_with_labels.tsv\")\n",
    "texts = [x[\"text\"] for x in dataset]\n",
    "y_true = [x[\"label\"] for x in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DIR / \"acled_label_to_name.json\") as f:\n",
    "    label_to_text = json.load(f)\n",
    "    \n",
    "label_names = sorted(label_to_text)\n",
    "label_texts = [label_to_text[l] for l in label_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZS_LABELS = [\"ORG_CRIME\", \"NATURAL_DISASTER\", \"MAN_MADE_DISASTER\", \"DIPLO\", \"ATTRIB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Label-Similarity Baseline\n",
    "\n",
    "A super simple zero-shot approach:\n",
    "* encode label names and the texts we want to classify in same embedding space\n",
    "* for a given text, assign label to whose embedding the text is closest using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_cosine(\n",
    "        model,\n",
    "        label_names,\n",
    "        label_texts=None,\n",
    "        input_texts=None,\n",
    "        input_embeddings=None,\n",
    "        label_embeddings=None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    * label_names: official names/ids of labels that are outputted as predictions\n",
    "    * label_texts: texts to build label representations from\n",
    "    \"\"\"\n",
    "\n",
    "    if label_embeddings is None:\n",
    "        label_embeddings = model.encode(label_texts)\n",
    "\n",
    "    if input_embeddings is None:\n",
    "        input_embeddings = model.encode(input_texts)\n",
    "        \n",
    "    S = util.pytorch_cos_sim(input_embeddings, label_embeddings)    \n",
    "    predicted_labels = []\n",
    "    \n",
    "    for i in range(len(input_embeddings)):\n",
    "        label_scores = S[i]                \n",
    "        scored = sorted(zip(label_names, label_scores), key=lambda x: x[1], reverse=True)\n",
    "        pred = scored[0][0]\n",
    "        predicted_labels.append(pred)        \n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "def evaluate(true_labels, pred_labels, label_set=None):\n",
    "    for avg in [\"micro\", \"macro\", \"weighted\"]:        \n",
    "        p, r, f, _ = precision_recall_fscore_support(true_labels, pred_labels, average=avg, labels=label_set)\n",
    "        gap = \" \" * (9 - len(avg))\n",
    "        print(f\"{avg}{gap}precision: {p:.3f}, recall: {r:.3f}, f-score: {f:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-Exemplars + KNN Approach\n",
    "\n",
    "1. Use above label-similarity approach to heuristically mine k exemplars for each label. \n",
    "\n",
    "2. Use these exemplars to build a KNN classifier\n",
    "\n",
    "In this experiment we will use the same test set to get initial predictions to mine exemplars, apply KNN and to evaluate our final predictions. <br> Note that we do not involve the real labels until the final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_exemplars(\n",
    "        model,\n",
    "        label_names,\n",
    "        label_texts=None,\n",
    "        input_texts=None,\n",
    "        input_embeddings=None,\n",
    "        label_embeddings=None,\n",
    "        k=10,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    For each label, collect k examples closest to the label embedding.\n",
    "    \"\"\"\n",
    "    if input_embeddings is None:\n",
    "        input_embeddings = model.encode(input_texts)\n",
    "    if label_embeddings is None:\n",
    "        label_embeddings = model.encode(label_texts)\n",
    "\n",
    "    S = util.pytorch_cos_sim(label_embeddings, input_embeddings)\n",
    "    label_to_exemplars = defaultdict(list)\n",
    "\n",
    "    n_labels, n_examples = S.shape\n",
    "    \n",
    "    for i in range(n_labels):\n",
    "        label = label_names[i]\n",
    "        scores = S[i]\n",
    "        scored_indices = zip(range(n_examples), scores)\n",
    "        scored_indices = sorted(\n",
    "            scored_indices,\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        for j, score in scored_indices[:k]:\n",
    "            exemplar = {\n",
    "                \"index\": j,\n",
    "                \"text\": input_texts[j],\n",
    "                \"score\": score\n",
    "            }\n",
    "            label_to_exemplars[label].append(exemplar)\n",
    "    return label_to_exemplars\n",
    "\n",
    "\n",
    "def classify_with_exemplars(\n",
    "        model, \n",
    "        label_names,\n",
    "        input_texts=None,        \n",
    "        input_embeddings=None,\n",
    "        label_to_exemplars=None,\n",
    "        knn=5,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Classify using weighted KNN based on a fixed small set of exemplars for \n",
    "    each class.\n",
    "    \"\"\"\n",
    "    if input_embeddings is None:\n",
    "        input_embeddings = model.encode(input_texts)\n",
    "\n",
    "    label_to_similarities = {}\n",
    "    for label, exemplars in label_to_exemplars.items():\n",
    "        texts = [x[\"text\"] for x in exemplars]\n",
    "        exemplar_embeddings = model.encode(texts)\n",
    "        label_to_similarities[label] = cosine_similarity(input_embeddings, exemplar_embeddings)\n",
    "    \n",
    "    predicted_labels = []\n",
    "    for i in range(len(input_embeddings)):\n",
    "        results = []\n",
    "        for label in label_names:\n",
    "            similarities = label_to_similarities[label]\n",
    "            for score in similarities[i]:\n",
    "                results.append((label, score))\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        label_to_score = defaultdict(float)\n",
    "        for label, score in results[:knn]:\n",
    "            label_to_score[label] += score\n",
    "        \n",
    "        top_label = max(label_to_score, key=label_to_score.get)\n",
    "        predicted_labels.append(top_label)\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "\n",
    "\n",
    "def classify_with_label_propagation(\n",
    "        model, \n",
    "        label_names,\n",
    "        input_texts=None,        \n",
    "        input_embeddings=None,\n",
    "        label_to_exemplars=None,\n",
    "    ):\n",
    "    \n",
    "    if input_embeddings is None:\n",
    "        input_len = len(input_texts)\n",
    "        input_embeddings = model.encode(input_texts)\n",
    "    else:\n",
    "        input_len = len(input_embeddings)\n",
    "    \n",
    "    label_prop_model = LabelSpreading()\n",
    "    labels = [-1] * input_len\n",
    "    \n",
    "    num_to_label = dict(enumerate(label_names))\n",
    "    label_to_num = dict((v, k) for k, v in num_to_label.items())\n",
    "    \n",
    "    for l, exemplars in label_to_exemplars.items():\n",
    "        for x in exemplars:\n",
    "            labels[x[\"index\"]] = label_to_num[l]\n",
    "    \n",
    "    label_prop_model.fit(input_embeddings, labels)\n",
    "    predicted_labels = label_prop_model.predict(input_embeddings)\n",
    "    predicted_labels = [num_to_label[num] for num in predicted_labels]\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 1.2.0, however, your version is 1.1.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"paraphrase-mpnet-base-v2\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_embeddings = model.encode(label_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = model.encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = classify_with_cosine(\n",
    "    model,\n",
    "    label_names,\n",
    "    label_embeddings=label_embeddings,\n",
    "    input_embeddings=input_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_exemplars = get_top_k_exemplars(\n",
    "    model,\n",
    "    label_names,\n",
    "    input_texts=texts,\n",
    "    label_embeddings=label_embeddings,\n",
    "    input_embeddings=input_embeddings,\n",
    "    k=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_ex = classify_with_exemplars(\n",
    "    model, \n",
    "    label_names,\n",
    "    input_embeddings=input_embeddings,\n",
    "    label_to_exemplars=label_to_exemplars,\n",
    "    knn=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demian/anaconda3/lib/python3.8/site-packages/sklearn/semi_supervised/_label_propagation.py:205: RuntimeWarning: invalid value encountered in true_divide\n",
      "  probabilities /= normalizer\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_lp = classify_with_label_propagation(\n",
    "    model, \n",
    "    label_names,\n",
    "    input_embeddings=input_embeddings,\n",
    "    label_to_exemplars=label_to_exemplars,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro    precision: 0.520, recall: 0.520, f-score: 0.520\n",
      "macro    precision: 0.528, recall: 0.495, f-score: 0.461\n",
      "weighted precision: 0.569, recall: 0.520, f-score: 0.489\n",
      "\n",
      "Zero shot labels only:\n",
      "\n",
      "micro    precision: 0.782, recall: 0.358, f-score: 0.491\n",
      "macro    precision: 0.871, recall: 0.383, f-score: 0.467\n",
      "weighted precision: 0.870, recall: 0.358, f-score: 0.431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demian/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_true, predicted_labels)\n",
    "print(\"\\nZero shot labels only:\\n\")\n",
    "evaluate(y_true, predicted_labels, label_set=ZS_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro    precision: 0.600, recall: 0.600, f-score: 0.600\n",
      "macro    precision: 0.579, recall: 0.590, f-score: 0.557\n",
      "weighted precision: 0.631, recall: 0.600, f-score: 0.585\n",
      "\n",
      "Zero shot labels only:\n",
      "\n",
      "micro    precision: 0.840, recall: 0.663, f-score: 0.741\n",
      "macro    precision: 0.854, recall: 0.684, f-score: 0.742\n",
      "weighted precision: 0.846, recall: 0.663, f-score: 0.723\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_true, predicted_labels_ex)\n",
    "print(\"\\nZero shot labels only:\\n\")\n",
    "evaluate(y_true, predicted_labels_ex, label_set=ZS_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro    precision: 0.310, recall: 0.310, f-score: 0.310\n",
      "macro    precision: 0.576, recall: 0.335, f-score: 0.381\n",
      "weighted precision: 0.623, recall: 0.310, f-score: 0.378\n",
      "\n",
      "Zero shot labels only:\n",
      "\n",
      "micro    precision: 0.816, recall: 0.326, f-score: 0.466\n",
      "macro    precision: 0.774, recall: 0.371, f-score: 0.485\n",
      "weighted precision: 0.741, recall: 0.326, f-score: 0.435\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_true, predicted_labels_lp)\n",
    "print(\"\\nZero shot labels only:\\n\")\n",
    "evaluate(y_true, predicted_labels_lp, label_set=ZS_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Exemplars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_exemplars(label_to_exemplars, dataset):\n",
    "    for l in sorted(label_to_exemplars):\n",
    "        exemplars = label_to_exemplars[l]\n",
    "        results = []\n",
    "        for x in exemplars:\n",
    "            i = x[\"index\"]            \n",
    "            if l == dataset[i][\"label\"]:\n",
    "                results.append(1)\n",
    "            else:\n",
    "                results.append(0)\n",
    "        acc = np.mean(results)\n",
    "        print(f\"Label: {l}, Accuracy: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: ABDUCT_DISSAP, Accuracy: 0.750\n",
      "Label: AGREEMENT, Accuracy: 0.600\n",
      "Label: AIR_STRIKE, Accuracy: 0.600\n",
      "Label: ARMED_CLASH, Accuracy: 0.700\n",
      "Label: ARREST, Accuracy: 0.500\n",
      "Label: ART_MISS_ATTACK, Accuracy: 0.650\n",
      "Label: ATTACK, Accuracy: 0.050\n",
      "Label: ATTRIB, Accuracy: 0.900\n",
      "Label: CHANGE_TO_GROUP_ACT, Accuracy: 0.200\n",
      "Label: CHEM_WEAP, Accuracy: 0.850\n",
      "Label: DIPLO, Accuracy: 0.300\n",
      "Label: DISR_WEAP, Accuracy: 0.550\n",
      "Label: FORCE_AGAINST_PROTEST, Accuracy: 0.300\n",
      "Label: GOV_REGAINS_TERIT, Accuracy: 0.600\n",
      "Label: GRENADE, Accuracy: 0.850\n",
      "Label: HQ_ESTABLISHED, Accuracy: 0.550\n",
      "Label: MAN_MADE_DISASTER, Accuracy: 0.300\n",
      "Label: MOB_VIOL, Accuracy: 0.350\n",
      "Label: NATURAL_DISASTER, Accuracy: 0.650\n",
      "Label: NON_STATE_ACTOR_OVERTAKES_TER, Accuracy: 0.350\n",
      "Label: NON_VIOL_TERRIT_TRANSFER, Accuracy: 0.250\n",
      "Label: ORG_CRIME, Accuracy: 0.750\n",
      "Label: OTHER, Accuracy: 0.050\n",
      "Label: PEACE_PROTEST, Accuracy: 0.900\n",
      "Label: PROPERTY_DISTRUCT, Accuracy: 0.650\n",
      "Label: PROTEST_WITH_INTER, Accuracy: 0.050\n",
      "Label: REM_EXPLOS, Accuracy: 0.550\n",
      "Label: SEX_VIOL, Accuracy: 0.950\n",
      "Label: SUIC_BOMB, Accuracy: 0.900\n",
      "Label: VIOL_DEMONSTR, Accuracy: 0.400\n"
     ]
    }
   ],
   "source": [
    "evaluate_exemplars(label_to_exemplars, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "* Two-pass classification with pseudo-exemplars works extremely well compared to default zero-shot baseline\n",
    "* Need to sanity-check this on other folds/datasets\n",
    "* While it adds running time within this test set, it is still a fast method since we can keep a fixed KNN classifier for future predictions \n",
    "* Actual label propagation algorithms from sklearn don't work great so far\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
