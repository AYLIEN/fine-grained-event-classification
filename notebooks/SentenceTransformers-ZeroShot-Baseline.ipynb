{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "def read_dataset(path):\n",
    "    \"\"\"Load tsv dataset from ACLED shared task.\"\"\"\n",
    "    with open(path) as f:\n",
    "        dataset = []\n",
    "        for line in list(f)[1:]:\n",
    "            id, text, label = line.strip().split(\"\\t\")\n",
    "            item = {\n",
    "                \"id\": id, \"text\": text, \"label\": label\n",
    "            }\n",
    "            dataset.append(item)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset and Labels\n",
    "\n",
    "We first load the following data from the ACLED shared task\n",
    "* `test_set_final_release_with_labels.tsv`: The test dataset, containing event descriptions and their labels\n",
    "* `label_to_description.json`: A mapping between between labels and label descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = Path(\"../data\")\n",
    "\n",
    "dataset = read_dataset(DIR / \"test_set_final_release_with_labels.tsv\")\n",
    "\n",
    "with open(DIR / \"label_to_description.json\") as f:\n",
    "    label_to_description = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By \"label description\" we mean a string of text that represents the meaning of a label that we want to predict. In zero-shot learning, these descriptions replace ground-truth examples of a class. The original label descriptions in the ACLED terminology are short phrases describing each concept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABDUCT_DISSAP': 'Abduction/forced disappearance',\n",
       " 'AGREEMENT': 'Agreement',\n",
       " 'AIR_STRIKE': 'Air/drone strike',\n",
       " 'ARMED_CLASH': 'Armed clash',\n",
       " 'ARREST': 'Arrests',\n",
       " 'ART_MISS_ATTACK': 'Shelling/artillery/missile attack',\n",
       " 'ATTACK': 'Attack',\n",
       " 'ATTRIB': 'Attribution of responsibility',\n",
       " 'CHANGE_TO_GROUP_ACT': 'Change to group/activity',\n",
       " 'CHEM_WEAP': 'Chemical weapon',\n",
       " 'DIPLO': 'Diplomatic event',\n",
       " 'DISR_WEAP': 'Disrupted weapons use',\n",
       " 'FORCE_AGAINST_PROTEST': 'Excessive force against protesters',\n",
       " 'GOV_REGAINS_TERIT': 'Government regains territory',\n",
       " 'GRENADE': 'Grenade',\n",
       " 'HQ_ESTABLISHED': 'Headquarters or base established',\n",
       " 'MAN_MADE_DISASTER': 'Man-made disaster',\n",
       " 'MOB_VIOL': 'Mob violence',\n",
       " 'NATURAL_DISASTER': 'Natural disaster',\n",
       " 'NON_STATE_ACTOR_OVERTAKES_TER': 'Non-state actor overtakes territory',\n",
       " 'NON_VIOL_TERRIT_TRANSFER': 'Non-violent transfer of territory',\n",
       " 'ORG_CRIME': 'Organized crime',\n",
       " 'OTHER': 'Other',\n",
       " 'PEACE_PROTEST': 'Peaceful protest',\n",
       " 'PROPERTY_DISTRUCT': 'Looting/property destruction',\n",
       " 'PROTEST_WITH_INTER': 'Protest with intervention',\n",
       " 'REM_EXPLOS': 'Remote explosive/landmine/IED',\n",
       " 'SEX_VIOL': 'Sexual violence',\n",
       " 'SUIC_BOMB': 'Suicide bomb',\n",
       " 'VIOL_DEMONSTR': 'Violent demonstration'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the useful bits from these files that we need later for classification and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [x[\"text\"] for x in dataset]\n",
    "y_true = [x[\"label\"] for x in dataset]\n",
    "\n",
    "label_names = sorted(label_to_description)\n",
    "label_descriptions = [label_to_description[l] for l in label_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following labels are used for a zero-shot evaluation in the shared task. Note that in this notebook, we predict all labels in a zero-shot fashion, but we still use the labels below to compare results of other systems in the shared task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZERO_SHOT_LABELS = [\"ORG_CRIME\", \"NATURAL_DISASTER\", \"MAN_MADE_DISASTER\", \"DIPLO\", \"ATTRIB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Simple Zero-Shot Classification\n",
    "\n",
    "Our approach in a nutshell:\n",
    "* We use a sentence encoder from `sentence-transformers` to convert both label descriptions and texts to predict into embeddings that live in the same embedding space.\n",
    "* At test time, we embed a new text and compare it to each label embedding via cosine similarity.\n",
    "* We assign the label with the highest similarity to the item.\n",
    "* Optionally, we define a minimum similarity threshold that a label needs to pass. If no label passes this threshold, we assign the \"OTHER\" class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotClassifier:\n",
    "    \n",
    "    def __init__(self, model=None, threshold=0.0, null_label=\"OTHER\"):\n",
    "        self.model = model\n",
    "        self.labels = []\n",
    "        self.label_embeddings = None\n",
    "        self.threshold = threshold\n",
    "        self.null_label = null_label\n",
    "    \n",
    "    def train(self, labels, descriptions):\n",
    "        self.labels = labels\n",
    "        self.label_embeddings = model.encode(descriptions)\n",
    "    \n",
    "    def predict(self, input_texts=None, input_embeddings=None, output_scores=False):\n",
    "        \n",
    "        if input_embeddings is None:\n",
    "            input_embeddings = self.model.encode(input_texts)\n",
    "            \n",
    "        S = util.pytorch_cos_sim(input_embeddings, self.label_embeddings)\n",
    "        \n",
    "        predicted_labels = []\n",
    "        predicted_scores = []\n",
    "        for i in range(input_embeddings.shape[0]):\n",
    "            label_scores = S[i].tolist()\n",
    "            scored = sorted(\n",
    "                zip(self.labels, label_scores),\n",
    "                key=lambda x: x[1],\n",
    "                reverse=True\n",
    "            )\n",
    "            pred, score = scored[0]\n",
    "            if score < self.threshold:\n",
    "                pred = self.null_label\n",
    "                \n",
    "            predicted_scores.append(scored)\n",
    "            predicted_labels.append(pred)        \n",
    "        \n",
    "        if output_scores:\n",
    "            return predicted_labels, predicted_scores\n",
    "        else:\n",
    "            return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcc0aad7f0b47b387e807d6062f6cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=690.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ba380721bf48d3bd3946f5e00c1988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3699.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc36134e0ad24dc78053350ebaabcd9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=594.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7465d28b8a0c46b290e8ffa24db72b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=122.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5603252ff04650baf8044fef4529a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=229.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa6b0c07d9b4e9db080a4a7c1408ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=438022897.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b1a23e703540d1bb4bea15c7ddb50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=53.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42589cc38514116938d21ee6c21a427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=239.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac0c965f888407db3c5f88367a2e9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466166.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da621cb6c6534122ae6e5118d8839a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1193.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ec93847bd04811b5ebced662e7a195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231536.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c940a17295fa43bf8a6c836c96126281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=190.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\" # set as \"cuda\" instead if you have a GPU set up\n",
    "model = SentenceTransformer(\"paraphrase-mpnet-base-v2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_classifier = ZeroShotClassifier(model=model)\n",
    "zs_classifier.train(labels=label_names, descriptions=label_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting and Evaluating Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_labels, pred_labels, label_set=None):\n",
    "    for avg in [\"micro\", \"macro\", \"weighted\"]:        \n",
    "        p, r, f, _ = precision_recall_fscore_support(\n",
    "            true_labels, pred_labels,\n",
    "            average=avg, labels=label_set, zero_division=0\n",
    "        )\n",
    "        gap = \" \" * (9 - len(avg))\n",
    "        print(f\"{avg}{gap}precision: {p:.3f}, recall: {r:.3f}, f-score: {f:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = zs_classifier.predict(input_texts=texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation on the entire test set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro    precision: 0.520, recall: 0.520, f-score: 0.520\n",
      "macro    precision: 0.528, recall: 0.495, f-score: 0.461\n",
      "weighted precision: 0.569, recall: 0.520, f-score: 0.489\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_true, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating only on subset of the labels that is used for zero-shot in the shared task:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro    precision: 0.782, recall: 0.358, f-score: 0.491\n",
      "macro    precision: 0.871, recall: 0.383, f-score: 0.467\n",
      "weighted precision: 0.870, recall: 0.358, f-score: 0.431\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_true, predicted_labels, label_set=ZERO_SHOT_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not perfect these results are pretty good given that we don't have any examples of the classes and only depend on short label descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your own Zero-Shot Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build a custom zero-shot classifier in a few lines of code!\n",
    "\n",
    "Let's say we're interested in a small number of natural disasters mentioned in news headlines: earthquakes, wildfires and floods. <br>\n",
    "We want our classifier to detect and classify these and label everything else as \"OTHER\".\n",
    "\n",
    "To do this, we set our classifier up with embeddings of very simple label descriptions (\"earthquake\", \"wildfire\", \"floods\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier = ZeroShotClassifier(model=model, threshold=0.3, null_label=\"OTHER\")\n",
    "\n",
    "my_classifier.train(\n",
    "    labels=[\"EARTHQUAKE\", \"WILDFIRE\", \"FLOODS\"],\n",
    "    descriptions=[\"earthquake\", \"wildfire\", \"floods\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FLOODS', 'WILDFIRE', 'EARTHQUAKE', 'OTHER', 'OTHER']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_classifier.predict([\n",
    "    \"Death toll from Hurricane Ida floods rises to 65 in US\",\n",
    "    \"As California burns, some ecologists say it’s time to rethink forest management\",\n",
    "    \"Maharashtra: Tremor in Kolhapur, no casualty\",\n",
    "    \"Leaked Guntrader firearms data file shared. Worst case scenario?\",\n",
    "    \"Taliban take control of last holdout in Panjshir Valley\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results look good!\n",
    "\n",
    "The examples for `WILDFIRE` and `EARTHQUAKE` demonstrate that we can correctly classify based on semantic proximity rather than literal word match.\n",
    "\n",
    "This is not going to work perfectly in all cases! But it's a good start for 1 minute of effort. To improve this approach you can tweak the label descriptions and the threshold. \n",
    "\n",
    "You can also use this approach to mine examples for each class you're interested for later manual verification, to build a dataset of ground-truth examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
