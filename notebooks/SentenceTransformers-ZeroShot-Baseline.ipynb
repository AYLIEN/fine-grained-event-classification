{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Event Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "def read_dataset(path):\n",
    "    \"\"\"Load tsv dataset from CASE 2021 shared task.\"\"\"\n",
    "    with open(path) as f:\n",
    "        dataset = []\n",
    "        for line in list(f)[1:]:\n",
    "            id, text, label = line.strip().split(\"\\t\")\n",
    "            item = {\n",
    "                \"id\": id, \"text\": text, \"label\": label\n",
    "            }\n",
    "            dataset.append(item)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset and Labels\n",
    "\n",
    "We first load the following data from the CASE 2021 Fine Grained Event classification shared task\n",
    "* `test_set_final_release_with_labels.tsv`: The test dataset, containing event descriptions and their labels\n",
    "* `label_to_description.json`: A mapping between between labels and label descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = Path(\"../data\")\n",
    "\n",
    "dataset = read_dataset(DIR / \"test_set_final_release_with_labels.tsv\")\n",
    "\n",
    "with open(DIR / \"label_to_description.json\") as f:\n",
    "    label_to_description = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By \"label description\" we mean a string of text that represents the meaning of a label that we want to be able to predict. In zero-shot learning, these descriptions can replace ground-truth labeled examples for a class. \n",
    "\n",
    "The original label descriptions in the ACLED event classification taxonomy are short phrases describing each concept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABDUCT_DISSAP': 'Abduction/forced disappearance',\n",
       " 'AGREEMENT': 'Agreement',\n",
       " 'AIR_STRIKE': 'Air/drone strike',\n",
       " 'ARMED_CLASH': 'Armed clash',\n",
       " 'ARREST': 'Arrests',\n",
       " 'ART_MISS_ATTACK': 'Shelling/artillery/missile attack',\n",
       " 'ATTACK': 'Attack',\n",
       " 'ATTRIB': 'Attribution of responsibility',\n",
       " 'CHANGE_TO_GROUP_ACT': 'Change to group/activity',\n",
       " 'CHEM_WEAP': 'Chemical weapon',\n",
       " 'DIPLO': 'Diplomatic event',\n",
       " 'DISR_WEAP': 'Disrupted weapons use',\n",
       " 'FORCE_AGAINST_PROTEST': 'Excessive force against protesters',\n",
       " 'GOV_REGAINS_TERIT': 'Government regains territory',\n",
       " 'GRENADE': 'Grenade',\n",
       " 'HQ_ESTABLISHED': 'Headquarters or base established',\n",
       " 'MAN_MADE_DISASTER': 'Man-made disaster',\n",
       " 'MOB_VIOL': 'Mob violence',\n",
       " 'NATURAL_DISASTER': 'Natural disaster',\n",
       " 'NON_STATE_ACTOR_OVERTAKES_TER': 'Non-state actor overtakes territory',\n",
       " 'NON_VIOL_TERRIT_TRANSFER': 'Non-violent transfer of territory',\n",
       " 'ORG_CRIME': 'Organized crime',\n",
       " 'OTHER': 'Other',\n",
       " 'PEACE_PROTEST': 'Peaceful protest',\n",
       " 'PROPERTY_DISTRUCT': 'Looting/property destruction',\n",
       " 'PROTEST_WITH_INTER': 'Protest with intervention',\n",
       " 'REM_EXPLOS': 'Remote explosive/landmine/IED',\n",
       " 'SEX_VIOL': 'Sexual violence',\n",
       " 'SUIC_BOMB': 'Suicide bomb',\n",
       " 'VIOL_DEMONSTR': 'Violent demonstration'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now extract the useful bits from the shared task files that we'll need later for classification and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [x[\"text\"] for x in dataset]\n",
    "y_true = [x[\"label\"] for x in dataset]\n",
    "\n",
    "label_names = sorted(label_to_description)\n",
    "label_descriptions = [label_to_description[l] for l in label_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot Labels\n",
    "\n",
    "The following subset of labels is used for a zero-shot evaluation in the CASE 2021 shared task. Note that we treat all labels in a zero-shot fashion, but for the purposes of the shared task we will do separate experiments on this subset to in order to compare results with other submissions in the shared task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_labels = [\"ORG_CRIME\", \"NATURAL_DISASTER\", \"MAN_MADE_DISASTER\", \"DIPLO\", \"ATTRIB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_zs, y_true_zs = zip(*[(text, label) for text, label in zip(texts, y_true) if label in zero_shot_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Simple Zero-Shot Classifier\n",
    "\n",
    "Our approach in a nutshell:\n",
    "* We use a sentence encoder from `sentence-transformers` to convert both label descriptions and texts to predict into embeddings that live in the same embedding space.\n",
    "* At test time, we embed a new text and compare it to each label embedding via cosine similarity.\n",
    "* We assign the label with the highest similarity to the item.\n",
    "* Optionally, we define a minimum similarity threshold that a label needs to pass. If no label passes this threshold, we assign the \"OTHER\" class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotClassifier:\n",
    "    \n",
    "    def __init__(self, model=None, threshold=None, null_label=\"OTHER\"):\n",
    "        self.model = model\n",
    "        self.labels = []\n",
    "        self.label_embeddings = None\n",
    "        self.threshold = threshold\n",
    "        self.null_label = null_label\n",
    "    \n",
    "    def train(self, labels, descriptions):\n",
    "        self.labels = labels\n",
    "        self.label_embeddings = model.encode(descriptions)\n",
    "    \n",
    "    def predict(self, input_texts=None, input_embeddings=None, output_scores=False):\n",
    "        \n",
    "        if input_embeddings is None:\n",
    "            input_embeddings = self.model.encode(input_texts)\n",
    "            \n",
    "        S = util.pytorch_cos_sim(input_embeddings, self.label_embeddings)\n",
    "        \n",
    "        predicted_labels = []\n",
    "        predicted_scores = []\n",
    "        for i in range(input_embeddings.shape[0]):\n",
    "            label_scores = S[i].tolist()\n",
    "            scored = sorted(\n",
    "                zip(self.labels, label_scores),\n",
    "                key=lambda x: x[1],\n",
    "                reverse=True\n",
    "            )\n",
    "            pred, score = scored[0]\n",
    "            if self.threshold is not None and score < self.threshold:\n",
    "                pred = self.null_label\n",
    "                \n",
    "            predicted_scores.append(scored)\n",
    "            predicted_labels.append(pred)        \n",
    "        \n",
    "        if output_scores:\n",
    "            return predicted_labels, predicted_scores\n",
    "        else:\n",
    "            return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" # set as \"cuda\" instead if you have a GPU set up\n",
    "# the first time this line runs the model will be downloaded \n",
    "model = SentenceTransformer(\"paraphrase-mpnet-base-v2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_classifier = ZeroShotClassifier(model=model)\n",
    "zs_classifier.train(labels=label_names, descriptions=label_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting and Evaluating Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_labels, pred_labels, label_set=None):\n",
    "    for avg in [\"micro\", \"macro\", \"weighted\"]:        \n",
    "        p, r, f, _ = precision_recall_fscore_support(\n",
    "            true_labels, pred_labels,\n",
    "            average=avg, labels=label_set, zero_division=0\n",
    "        )\n",
    "        gap = \" \" * (9 - len(avg))\n",
    "        print(f\"{avg}{gap}precision: {p:.3f}, recall: {r:.3f}, f-score: {f:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = zs_classifier.predict(input_texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember, as mentioned above, we're doing zero-shot classification everywhere\n",
    "# in this system, but we separated the label sets to fit the shared-task evaluation \n",
    "# setup\n",
    "predicted_labels_zs = zs_classifier.predict(input_texts=texts_zs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on the entire test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro    precision: 0.520, recall: 0.520, f-score: 0.520\n",
      "macro    precision: 0.528, recall: 0.495, f-score: 0.461\n",
      "weighted precision: 0.569, recall: 0.520, f-score: 0.489\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_true, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating only on subset of the labels that is used for zero-shot in the shared task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro    precision: 0.840, recall: 0.358, f-score: 0.502\n",
      "macro    precision: 0.914, recall: 0.383, f-score: 0.477\n",
      "weighted precision: 0.920, recall: 0.358, f-score: 0.443\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_true_zs, predicted_labels_zs, label_set=zero_shot_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is room for improvement, these results are pretty good given that we didn't use any training data at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Building your own Zero-Shot Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build a custom zero-shot classifier in a few lines of code!\n",
    "\n",
    "Let's say we're interested in a small number of natural disasters mentioned in news headlines: earthquakes, wildfires and floods. <br>\n",
    "We want our classifier to detect and classify these and label everything else as \"OTHER\".\n",
    "\n",
    "To do this, we set our classifier up with embeddings of very simple label descriptions (\"earthquake\", \"wildfire\", \"floods\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier = ZeroShotClassifier(\n",
    "    model=model,\n",
    "    threshold=0.3,    \n",
    "    null_label=\"OTHER\"\n",
    ")\n",
    "\n",
    "my_classifier.train(\n",
    "    labels=[\"EARTHQUAKE\", \"WILDFIRE\", \"FLOODS\"],\n",
    "    descriptions=[\"earthquake\", \"wildfire\", \"floods\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the classifier to some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FLOODS', 'WILDFIRE', 'EARTHQUAKE', 'OTHER', 'OTHER']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_classifier.predict([\n",
    "    \"Death toll from Hurricane Ida floods rises to 65 in US\",\n",
    "    \"As California burns, some ecologists say it’s time to rethink forest management\",\n",
    "    \"Maharashtra: Tremor in Kolhapur, no casualty\",\n",
    "    \"Leaked Guntrader firearms data file shared. Worst case scenario?\",\n",
    "    \"Taliban take control of last holdout in Panjshir Valley\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results look good!\n",
    "\n",
    "The test examples for `WILDFIRE` and `EARTHQUAKE` above demonstrate that we can correctly classify based on semantic proximity rather than literal word match.\n",
    "\n",
    "This is not going to work perfectly in all cases! But it's a good start for 1 minute of effort. To improve this approach you can tweak the label descriptions and the threshold. \n",
    "\n",
    "You can also use this approach to mine examples for each class you're interested for later manual verification, to build a dataset of ground-truth examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Example with fine-grained event types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier = ZeroShotClassifier(\n",
    "    model=model,\n",
    "    threshold=0.2,    \n",
    "    null_label=\"OTHER\"\n",
    ")\n",
    "\n",
    "my_classifier.train(\n",
    "    labels=[\"COMP-ACQUISITION\", \"STAKE-ACQUISITION\"],\n",
    "    descriptions=[\n",
    "        \"Company acquires other company\",\n",
    "        \"Company buys stocks/stake in other company\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STAKE-ACQUISITION',\n",
       " 'STAKE-ACQUISITION',\n",
       " 'STAKE-ACQUISITION',\n",
       " 'COMP-ACQUISITION',\n",
       " 'COMP-ACQUISITION',\n",
       " 'COMP-ACQUISITION']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_classifier.predict([\n",
    "    \"Galetech Group buys majority stake in Optinergy\",\n",
    "    \"SoftBank acquires minor stake in Deutsche Telekom in new 'long-term partnership'\",\n",
    "    \"EQT buys stake in Sweden's Storytel, becomes second largest shareholder\",\n",
    "    \"UK’s Digital 9 Infrastructure acquires Verne Global for €269.1M; here’s why\",\n",
    "    \"French technology company Lectra acquires Gemini CAD systems\",\n",
    "    \"Quercus buys Arcadia Books as Bielenberg named publisher\",\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
